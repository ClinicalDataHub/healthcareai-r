% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_models.R
\name{tune_models}
\alias{tune_models}
\title{Identify the best performing model by tuning hyperparameters via
cross-validation}
\usage{
tune_models(d, outcome, model_class, models = c("rf", "knn"), n_folds = 5,
  tune_depth = 10, tune_method = "random", metric, hyperparameters)
}
\arguments{
\item{d}{A data frame}

\item{outcome}{Name of the column to predict}

\item{model_class}{One of "regression", "classification", "multiclass", or
"unsupervised", but only regression and classification are currently
supported.}

\item{models}{Names of models to try, by default for regression and
classification "rf" for random forest and "knn" for k-nearest neighbors.
See \code{\link{supported_models}} for available models.}

\item{n_folds}{How many folds to use in cross-validation? Default = 5.}

\item{tune_depth}{How many hyperparameter combinations to try? Defualt = 10.}

\item{tune_method}{How to search hyperparameter space? Only "random" is
currently supported. Eventually, "random" (default) or "grid".}

\item{metric}{What metric to use to assess model performance? Options for
regression: "RMSE" (root-mean-squared error, default), "MAE" (mean-absolute
error), or "Rsquared." For classification: "ROC" (area under the receiver
operating characteristic curve).}

\item{hyperparameters}{Currently not supported. Optional. A list of
hyperparameter values to tune over. Overrides \code{tune_depth}. A list of
lists. The names of the outer-list must match \code{models}. The names of
each inner-list must match the hyperparameters available to tune over for
the respective model. Entries in each inner-list are the values of the
hyperparameter to try. These will be expanded to run a full grid search
over every combination of values. For details on support models and
hyperparameters see \code{\link{supported_models}}.}
}
\value{
A model_list object
}
\description{
Identify the best performing model by tuning hyperparameters via
cross-validation
}
\details{
Note that in general a model is trained for each hyperparameter
  combination in each fold for each model, so run time is a function of
  length(models) x n_folds x tune_depth.
}
\examples{
\dontrun{
### Takes ~7 seconds
# Remove identifier variables and rows with missingness,
# and choose 100 rows to speed tuning
d <-
  pima_diabetes \%>\%
  dplyr::select(-patient_id) \%>\%
  stats::na.omit() \%>\%
  dplyr::sample_n(100)
m <- tune_models(d, outcome = diabetes, model_class = "classification")
# Plot performance over hyperparameter values for each algorithm
plot(m)
# Extract confusion matrix for KNN
caret::confusionMatrix(m[[2]], norm = "none")
# Compare performance of algorithms at best hyperparameter values
rs <- resamples(m)
dotplot(rs)
}
}
\seealso{
\code{\link{prep_data}}, \code{\link{predict.model_list}}
}
