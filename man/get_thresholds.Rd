% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_thresholds.R
\name{get_thresholds}
\alias{get_thresholds}
\title{Get class-separating thresholds for classification predictions}
\usage{
get_thresholds(x, measures = c("cost", "acc", "tpr", "fnr", "tnr", "fpr",
  "ppv", "npv"), cost.fp = 1, cost.fn = 1)
}
\arguments{
\item{x}{Either a predictions data frame (from \code{predict}) or a
model_list (e.g. from \code{machine_learn}).}

\item{measures}{Character vector of performance metrics to calculate. The
returned data frame will have one column for each metric. Any
  of the non-scaler measure arguements to \code{ROCR::performance} should work.
  Defaults to all of the following: \itemize{
  \item{cost: Captures how bad all the errors are. You can adjust the relative costs
   of false alarms and missed detections by setting \code{cost.fp} or
   \code{cost.fn}}. At the default of equal costs, this is directly inversely
   proportional to accuracy.
  \item{acc: Accuracy}
  \item{tpr: True positive rate, aka sensitivity, aka recall}
  \item{tnr: True negative rate, aka specificity}
  \item{fpr: False positive rate, aka fallout}
  \item{fnr: False negative rate}
  \item{ppv: Positive predictive value, aka precision}
  \item{npv: Negative predictive value}
  }}

\item{cost.fp}{Cost of a false positive. Default = 1. Only affects cost.}

\item{cost.fn}{Cost of a false negative. Default = 1. Only affects cost.}
}
\value{
Tibble with rows for each possible threshold
and columns for the thresholds and each value in \code{measures}.
}
\description{
healthcareai gives you predicted probabilities for classification
problems, but sometimes you may need to convert probabilities into predicted
classes. That requires choosing a threshold, where probabilities above the
threshold are predicted as the positive class and probabilities below the
threshold are predicted as the negative class. This function helps you do that
by calculating a bunch of model-performance metrics at every possible
threshold.

We recommend plotting the thresholds with their performance measures to
see how optimizing for one measure affects performance on other measures.
See \code{\link{plot.thresholds_df}} for how to do this.
}
\examples{
models <- machine_learn(pima_diabetes[1:20, ], patient_id, outcome = diabetes,
                        models = "rf", tune = FALSE)

# Get all the possible thresholds and performance measures at each
(thresholds <- get_thresholds(models))

# Extract the threshold that makes the highest accuracy predictions and
# use it to generate predicted classes on the training dataset.
library(dplyr)
optimal_threshold <- thresholds$threshold[which.max(thresholds$acc)]
predict(models) \%>\%
  mutate(predicted_class_diabetes = case_when(
    predicted_diabetes > optimal_threshold ~ "Y",
    predicted_diabetes <= optimal_threshold ~ "N"
  )) \%>\%
  select_at(vars(ends_with("diabetes")))
}
